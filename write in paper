“While the proposed model achieves high performance with Adam, its performance degrades significantly with AdaBoB. This suggests that 
the model’s convergence is sensitive to gradient adaptivity and bounded learning rate constraints, indicating potential over-reliance 
on Adam’s aggressive learning dynamics. Future work will investigate optimizer-agnostic robustness.”


We evaluated all SOTA baselines using publicly released pretrained checkpoints and conducted inference with our test protocol. Re-training all baselines under modern bounded optimizers (e.g. AdaBoB/AdaBound) would require reimplementing and fine-tuning multiple codes and large compute, which was infeasible within our revision timeline. To address the reviewer’s concern about optimizer sensitivity, we conducted a controlled optimizer ablation on our method: we fine-tuned our model under Adam and a bounded optimizer (AdaBound/AdaBoB) after performing a small learning-rate sweep and running 3 independent seeds per condition. Results (Table X and Fig. Y) show that while our model achieves best numbers with Adam, performance degrades under bounded optimizers despite hyperparameter tuning. We include training/validation curves and the exact optimizer hyperparameters in the Appendix. We therefore report optimizer sensitivity as an empirical limitation and leave re-training all SOTA baselines under AdaBoB for future work.